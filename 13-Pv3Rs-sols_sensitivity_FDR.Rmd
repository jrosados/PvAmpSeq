---
title: "Running Pv3Rs for Solomon Islands data"
author: "Shazia Ruybal-Pes√°ntez"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide 
    code_download: true
    fig_width: 8
    fig_height: 6
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(janitor)
library(here)
library(patchwork)
library(RColorBrewer)
library(reshape2)

# install.packages('Pv3Rs', repos = c('https://plasmogenepi.r-universe.dev', 'https://cloud.r-project.org'))

# devtools::install_github("aimeertaylor/Pv3Rs")
# Pv3Rs downloaded from Github on 7 Aug 2024
library(Pv3Rs)

set.seed(55)
```

## Set global variables
From TO/JS - This section sets some flags that can be tweaked to re-run the analysis with different filtering parameters:

`RUN_EXAMPLE`: allows discarding alleles that have a frequency below a threshold (defaults to FALSE),
`WITHIN_INDIVIDUAL_ALLELE_FREQ_THR`: allows discarding alleles that have a frequency below a threshold (defaults to 0),
`BENCHMARK_MARKERS`: list of markers to include in the analysis (defaults to the full list established by Jason, could be reduced down to 3 for memory optimization),
`MAX_MOI_TO_INCLUDE`: allows discarding participant data when they exhibit very complex infection patterns, counted as the total number of clones across all infection events (defaults to 8),
`PRIOR_3RS`: a named vector with prior probabilities of recrudescence (C), relapse (L) or reinfection (I) (defaults to uniform, i.e. 1/3 each, but tweaked to minimize recrudescence in the present study).

```{r global vars}
## RUN_EXAMPLE
##   Boolean flag to enable/disable running the example Pv3Rs posterior 
##   computation on a single episode sampled at random from the dataset.
## DEFAULT: FALSE
RUN_EXAMPLE <- TRUE

## WITHIN_INDIVIDUAL_ALLELE_FREQ_THR
##   Minimum threshold above which an allele is preserved in individual
##   haplotype data to be preserved when reconstructing infection 
##   history. Note that this will *not* discard these alleles from the
##   population-level allele frequency that is derived from the initial
##   visit, but only discard that allele from individual observations 
##   when it is 'too rare to be exploited'.
## DEFAULT: 0
WITHIN_INDIVIDUAL_ALLELE_FREQ_THR <- 0

## BENCHMARK_MARKERS
##   Vector of character string with names matching those from the 
##   markers of interest for amplicon sequencing. Only the markers 
##   listed in that vector will be preserved in individual-level data
##   and identity of infection will be solely based on the alleles 
##   observed for these markers.
##   The markers are listed by importance as discovered by Jason.
## DEFAULT: all markers (could be suboptimal/too memory-consuming?)
BENCHMARK_MARKERS <- c(
  "Chr05",
  "Chr07", 
  "Chr09", 
  "Chr10",
  "Chr08",
  "Chr13",
  "Chr11",
  "Chr03",
  "Chr01",
  "Chr02",
  "Chr14"
)

## MAX_MOI_TO_INCLUDE
##   As per Aimee's guidelines: 
##     We do not recommend running compute_posterior() for data 
##     whose total genotype count exceeds eight, where the total 
##     genotype count is the sum of per-episode maximum per-marker 
##     allele counts.
##   The MAX_MOI_TO_INCLUDE expects an integer that will discard all
##   individuals having a summed MOI > 8 across all recorded episodes.
## DEFAULT: 8
MAX_MOI_TO_INCLUDE <- 8

## PRIORS_3RS
##   A vector of probabilities, summing to 1, corresponding to 
##   the probability of each stage for the 3Rs for Pv episodes.
##   The vector order is re(C)rudescence, re(L)apse, re(I)nfection.
##   In this clinical trial, we assume recrudescence is possible so, 
##   we use the default priors 
## DEFAULT: c("C" = 1/3, "L" = 1/3, "I" = 1/3)
PRIOR_3RS <- c("C" = 1/3, "L" = 1/3, "I" = 1/3)
# Note: the below probabilities were used for SeroTAT study because it does not involve treatment at baseline
# PRIOR_3RS <- c("C" = 0.10, "L" = 0.45, "I" = 0.45)
```

## Solomon Islands data curation

Read in data from Solomon Islands:
```{r load data}
sols <- read.csv(here("data/final", "sols_raw_data.csv"))

# head(sols)
# names(sols)
```

### True recurrences
Participants with more than one episode for inference (n=41 participants, n=99 Pv isolates):
```{r recurrences}
# sols %>% 
#   select(info, episodes) %>% 
#   distinct() %>% 
#   arrange(info) %>% 
#   filter(episodes>1) 

ids_recurrent <- sols %>% clean_names() %>% filter(episodes > 1) %>% distinct(info)
sols_recurrent <- sols %>% clean_names() %>% filter(episodes > 1)

# setdiff(ids_recurrent$info, sols_recurrent$info)
# setdiff(sols_recurrent$info, ids_recurrent$info)

# sols_recurrent %>% distinct(sample) %>% arrange(sample) # 99 isolates
```

```{r plot recurrences, fig.height=8, fig.width=8}
sols_recurrent %>% 
  distinct(sample) %>% 
  separate(sample, into = c("sample", "episode_day"), sep = "-(?=[0-9]+$)") %>% 
  left_join(sols_recurrent %>% distinct(info, trt_label, trtgrp), by = c("sample" = "info")) %>%  
  ggplot(aes(x = as.numeric(episode_day), y = reorder(sample, as.numeric(episode_day)))) +
    geom_line(aes(group = sample), color = "darkgrey") +
    geom_point() +
    scale_x_continuous(breaks = scales::pretty_breaks(n=10)) +
    labs(x = "time since baseline episode",
         y = "sample") +
    theme_bw() +
    facet_grid(trt_label~., scales = "free_y") 
```

### Data on episode number and time since last episode
```{r episode summary}
episode_summary <- sols_recurrent %>% 
  distinct(info, info_d, vis_date, sample) %>% 
  mutate(vis_date = mdy(vis_date)) %>%
  arrange(info, vis_date) %>% 
  group_by(info) %>% 
  mutate(
    # get the episode number
    episode_number = row_number(),
    # calculate days since enrolment, just a check with dates
    days_since_enrolment = as.integer(vis_date - min(vis_date[info_d == 0])),
    # calculate days since last episode using lag() and making enrolment episodes 0 days since last
    days_since_last_episode = replace_na(as.integer(vis_date - lag(vis_date)), 0)
  ) %>% 
  ungroup()
```

### "Null distribution"
To understand the false discovery rate (FDR), we will 'jumble up' the pairs of recurrences so that they are matched to the wrong baseline episode and recurrent episode (i.e. in different people) and then run `compute_posterior()` to see what the Pr(false recrudescence) and Pr(false relapse) based on our dataset.

First will start with the simplest random pairing:

- every baseline episode will have one recurrent episode pair at random (based on all possible recurrent episodes (n=58))
- we don't keep any of the original structure in the data wrt treatment (*- Should I impose grouping by treatment?*)

There are 77 participants (36 had only the baseline episode and 41 of them had at least 1 recurrent episode post-treatment).
```{r}
sols_all_patients <- sols %>%
        clean_names() %>% 
        select(sample, info, info_d, episode_type = follow_x, episodes, trtgrp, trt_label, vis_date) %>% 
        distinct() %>% 
        arrange(sample)

sols_all_patients %>% 
  distinct(info, episodes) %>%
  ggplot(aes(x = episodes)) +
    geom_bar() +
    scale_x_continuous(breaks = scales::pretty_breaks(n=5)) +
    theme_bw()
```

```{r fxn to randomize pairs}
randomize_pairs <- function(data) {
  max_attempts <- 100 
  attempt <- 1
  repeat {
    # shuffle samples within each episode type (baseline and follow-up) and assign pair_index
    shuffled_list <- data %>%
      group_by(episode_type) %>%
      group_split() %>% 
      lapply(function(x) {
        x %>% 
          mutate(random_order = sample(n())) %>%
          arrange(random_order) %>% 
          mutate(pair_index = row_number()) %>%
          select(-random_order)
      })
    
    # bind baseline and follow-ups
    shuffled_data <- bind_rows(shuffled_list)
    
    # Check for duplicate 'info' within pair_index
    duplicates <- shuffled_data %>%
      group_by(pair_index) %>%
      filter(n() > 1) %>%  # only consider pair_indices with more than one sample
      filter(n_distinct(info) < n()) %>%
      ungroup()
    
    # Break the loop if no duplicates are found
    if (nrow(duplicates) == 0) {
      # assign a new pair ID 
      shuffled_data <- shuffled_data %>% 
                        mutate(shuffled_pair_id = paste0("NewID-", pair_index)) %>%
                        # only keep paired samples
                        group_by(pair_index) %>% 
                        filter(n() > 1) %>% 
                        ungroup()
      return(shuffled_data)
    }
    
    # Increment attempt counter
    attempt <- attempt + 1
    if (attempt > max_attempts) {
      stop("Unable to find a pairing without duplicate 'info' after multiple attempts.")
    }
  }
}
```

```{r randomize pairs}  
random_pairs <- randomize_pairs(sols_all_patients)

# check again that we haven't randomly matched to the same patient!
# random_pairs %>% 
#   group_by(shuffled_pair_id) %>%
#   filter(n_distinct(info) < n()) %>%
#   summarise(duplicate_info = list(info)) %>%
#   ungroup()
```

Merge back with full dataset
```{r merge with full}
sols_recurrent_null <- random_pairs %>% 
  left_join(sols %>% clean_names(), by = c("sample", "info", "info_d", "episodes", "trtgrp", "trt_label", "vis_date")) 
```

These are our 'new' shuffled pairs:
```{r plot shuffled recurrences, fig.height=8, fig.width=8}
sols_recurrent_null %>% 
  ggplot(aes(x = info_d, y = shuffled_pair_id)) +
    geom_line(aes(group = shuffled_pair_id), color = "darkgrey") +
    geom_point() +
    scale_x_continuous(breaks = scales::pretty_breaks(n=10)) +
    labs(x = "time since baseline episode",
         y = "sample") +
    theme_bw() 
```

### Data on episode number and time since last episode for new shuffled pairs
```{r episode summary null}
episode_summary_null <- sols_recurrent_null %>% 
  distinct(shuffled_pair_id, info_d, vis_date, sample) %>% 
  mutate(vis_date = mdy(vis_date)) %>%
  arrange(shuffled_pair_id, vis_date) %>% 
  group_by(shuffled_pair_id) %>% 
  mutate(
    # get the episode number
    episode_number = row_number(),
    # calculate days since enrolment, just a check with dates
    days_since_enrolment = as.integer(vis_date - min(vis_date[info_d == 0])),
    # calculate days since last episode using lag() and making enrolment episodes 0 days since last
    days_since_last_episode = replace_na(as.integer(vis_date - lag(vis_date)), 0)
  ) %>% 
  ungroup()
```

## Haplotype frequencies
We want to remove any haplotypes that appear only once at very low frequencies. Haplotypes should already be filtered to be observed in at least 2 samples and within-host frequency >=1% (in full dataset).

```{r singletons}
singleton_haps <- sols_recurrent_null %>% 
  select(sample, marker_id, haplotype, frequency, count) %>% 
  count(haplotype) %>% 
  arrange(n) %>% 
  filter(n==1) %>% 
  pull(haplotype)

sols_recurrent %>%
  filter(haplotype %in% singleton_haps) %>%
  ggplot(aes(x = haplotype, y = count)) +
    geom_hline(yintercept = 100, linetype = "dashed") +
    geom_point(aes(color = frequency, shape = follow_x), size = 3) +
    labs(x = "singleton haplotype",
         y = "read count",
         color = "within-sample frequency",
         shape = "timepoint") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

## Haplotypes for inference
As per Aimee, to avoid bias due to within-host selection of recrudescent parasites, we use only enrolment episodes to estimate population-level alelle frequencies. So, we will filter out any haplotypes that do not appear at baseline (drop n=10 haplotypes). Details below on their within-sample frequencies and read counts.
```{r haps for inference}
# n=741 haps
baseline_haps <- sols_recurrent_null %>% 
  filter(episode_type == "baseline") %>% 
  pull(haplotype)

sols_recurrent %>%
  filter(!haplotype %in% baseline_haps) %>%
  select(sample, marker_id, haplotype, frequency, count) %>%
  arrange(frequency, count)
```

## Final data for inference
```{r final analysis data}
analysis_data <- sols_recurrent_null %>%
  select(subject_id = shuffled_pair_id, # using "new ID" instead of original participant ID
         sample_id = sample, 
         treatment_arm = trt_label, 
         days_since_treatment = info_d,
         timepoint = follow_x,
         visit_date = vis_date,
         age_years = age_y, 
         sex = gender, 
         # episodes, # this is not accurate anymore
         marker_id, 
         haplotype, 
         type,
         frequency,
         mean_moi,
         max_moi) %>% 
  # add extra epi info on episode number and time since last episode
  left_join(episode_summary_null %>% select(subject_id = shuffled_pair_id, 
                                       sample_id = sample, 
                                       episode_number,
                                       days_since_enrolment,
                                       days_since_last_episode),
            by = c("subject_id", "sample_id"))  %>% 
  # keep only haplotypes present at baseline
  filter(haplotype %in% baseline_haps) %>% 
  # ensure dates are date class
  mutate(visit_date = mdy(visit_date)) 
```

### Baseline allele frequencies based on day 0 samples
```{r baseline allele freqs}
## Derive baseline allele frequency - This is modified from Thomas/Jason script for our data
baseline_fs <- analysis_data %>% 
  
  # Use only baseline samples
  filter(timepoint == "baseline") %>% 
  
  # split data frame by marker
  group_by(marker_id) %>% 
  group_split() %>% 
  
  # Derive a within-marker list of frequencies, by individual
  lapply(function(x) {
    x <- x %>% 
      # Build a within-individual frequency table that 
      # always includes every haplotype (even the ones absent)
      mutate(haplotype = factor(haplotype)) %>% 
      select(sample_id, haplotype, frequency) %>% 
      pivot_wider(names_from = haplotype, 
                  values_from = frequency, 
                  values_fill = 0) %>% 
      pivot_longer(cols = -sample_id, 
                   names_to = "haplotype", 
                   values_to = "frequency") %>% 
      # Get population-level haplotype frequency, 
      # correcting for when within-individual sum is not equal 
      # to 1, as can happen when a minority clone is <2%
      group_by(sample_id) %>% 
      mutate(frequency = frequency / sum(frequency, na.rm = TRUE)) %>% 
      group_by(haplotype) %>% 
      summarise(frequency_pop_mean = mean(frequency, na.rm = TRUE))
    
    return(deframe(x))
  }) %>% 
  # get marker_id from group_keys from the group dfs 
  setNames(nm = analysis_data %>% group_by(marker_id) %>% group_keys() %>% pull(marker_id))

# Here we can also save as dataframe for easier printing and table-ready for paper
baseline_fs_df <- analysis_data %>% 
  # Use only baseline samples
  filter(timepoint == "baseline") %>% 
  
  # Group by marker_id and sample_id for further calculations
  group_by(marker_id, sample_id) %>% 
  
  # Build a within-individual frequency table that always includes every haplotype (even the ones absent)
  mutate(haplotype = factor(haplotype)) %>% 
  select(marker_id, sample_id, haplotype, frequency) %>% 
  pivot_wider(names_from = haplotype, values_from = frequency, values_fill = list(frequency = 0)) %>% 
  pivot_longer(cols = -c(marker_id, sample_id), names_to = "haplotype", values_to = "frequency") %>% 
  
  # Get population-level haplotype frequency, correcting for when within-individual sum is not equal to 1
  group_by(marker_id, sample_id) %>% 
  mutate(frequency = frequency / sum(frequency, na.rm = TRUE)) %>% 
  group_by(marker_id, haplotype) %>% 
  summarise(frequency_pop_mean = mean(frequency, na.rm = TRUE), .groups = 'drop') %>% 
  
  # Ensure haplotypes are correctly associated with their marker_id - note that this works for us because , in future would have to make this flexible to allow for haplotype names that are not reliant on having marker_id
  filter(str_detect(haplotype, marker_id))

# baseline_fs_df
```

## Pv3Rs
### Example on one participant: AR-067
```{r pv3rs one example}
# This has been modified from Thomas/Jason script 

## Pick an individual at random to run Pv3Rs
# indiv_name <- sample(unique(analysis_data$subject_id), size = 1)
indiv_name <- "NewID-31"

## Prepare the data
# 1- Subset haplotype data to specific individual and apply filters
indiv_haplotype_data <- analysis_data %>% 
  # Restrict to a single patient
  filter(subject_id == indiv_name) %>% 
  # Restrict to a subset of markers, if needed
  filter(marker_id %in% BENCHMARK_MARKERS) %>% 
  # Restrict to summed MOI below threshold
  group_by(subject_id, episode_number, marker_id) %>% 
  mutate(MOI_per_marker = sum(n())) %>% 
  group_by(subject_id, episode_number) %>% 
  mutate(MOI_per_episode = max(MOI_per_marker, na.rm = TRUE)) %>% 
  ungroup() %>% 
  mutate(marker_id = factor(marker_id, levels = BENCHMARK_MARKERS))

# 2- Calculate per-episode and per-participant MOI for PvR3S eligibility
indiv_MOI <- indiv_haplotype_data %>% 
  select(subject_id, episode_number, 
         marker_id, starts_with("MOI_")) %>% 
  distinct() %>% 
  group_by(subject_id, episode_number) %>% 
  # Get highest per-marker MOI only for each episode
  # (drop marker_id in case of ties with highest per-marker MOI)
  select(-marker_id) %>% 
  distinct() %>% 
  filter(MOI_per_marker == max(MOI_per_marker, na.rm = TRUE)) %>% 
  ungroup() %>% 
  mutate(MOI_summed = sum(MOI_per_episode, na.rm = TRUE))
```

Prepare test data
```{r}
# Finish data preparation
  indiv_haplotype_data <- indiv_haplotype_data %>% 
    group_by(episode_number) %>% 
    group_split() %>% 
    lapply(function(x) {
      res <- x %>% 
        select(sample_id, episode_number, 
               marker_id, haplotype, frequency) %>% 
        # For sensitivity analysis, allow to include/drop allele 
        # based on their within-individual frequency
        filter(frequency >= WITHIN_INDIVIDUAL_ALLELE_FREQ_THR) %>% 
        select(-sample_id, -episode_number, -frequency) %>% 
        distinct() %>% 
        # Prevent dropping of markers that are not characterised 
        # by setting .drop to FALSE
        group_by(marker_id, .drop = FALSE) %>% 
        group_split() %>% 
        lapply(function(y) {
          unique(y$haplotype)
        })
      
      # Returned a list named with each episode, 
      # setting marker allele to NA in case none are observed
      return(lapply(setNames(res, BENCHMARK_MARKERS), 
                    function(y) {
                      if (length(y) == 0) return(NA) else return(y)
                    }))
    })
```

```{r run compute posterior one indiv}
# Run Aimee's posterior estimation
indiv_posterior <- Pv3Rs::compute_posterior(y  = indiv_haplotype_data, 
                                            fs = baseline_fs[BENCHMARK_MARKERS])

indiv_posterior
```

The joint posterior probability of reinfection is 0.818%, but the model finds probability of relapse to be 18.2% (false positive).
```{r plot haps}
sols_recurrent_null %>% 
  filter(shuffled_pair_id == "NewID-31") %>% 
  ggplot(aes(x = haplotype, y = factor(info_d), fill = factor(haplotype))) +
  geom_tile() +
    facet_grid(~marker_id, scales = "free_x") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```
### Run Pv3Rs one random individual
```{r subset haplotype data at random}
# This has been modified from Thomas/Jason script 

## Pick an individual at random to run Pv3Rs
indiv_name <- sample(unique(analysis_data$subject_id), size = 1)

## Prepare the data
# 1- Subset haplotype data to specific individual and apply filters
indiv_haplotype_data <- analysis_data %>% 
  # Restrict to a single patient
  filter(subject_id == indiv_name) %>% 
  # Restrict to a subset of markers, if needed
  filter(marker_id %in% BENCHMARK_MARKERS) %>% 
  # Restrict to summed MOI below threshold
  group_by(subject_id, episode_number, marker_id) %>% 
  mutate(MOI_per_marker = sum(n())) %>% 
  group_by(subject_id, episode_number) %>% 
  mutate(MOI_per_episode = max(MOI_per_marker, na.rm = TRUE)) %>% 
  ungroup() %>% 
  mutate(marker_id = factor(marker_id, levels = BENCHMARK_MARKERS))

# 2- Calculate per-episode and per-participant MOI for PvR3S eligibility
indiv_MOI <- indiv_haplotype_data %>% 
  select(subject_id, episode_number, 
         marker_id, starts_with("MOI_")) %>% 
  distinct() %>% 
  group_by(subject_id, episode_number) %>% 
  # Get highest per-marker MOI only for each episode
  # (drop marker_id in case of ties with highest per-marker MOI)
  select(-marker_id) %>% 
  distinct() %>% 
  filter(MOI_per_marker == max(MOI_per_marker, na.rm = TRUE)) %>% 
  ungroup() %>% 
  mutate(MOI_summed = sum(MOI_per_episode, na.rm = TRUE))
```


Now that the data has been subset, Pv3Rs::compute_posterior() will be called (only if the participant meets eligibility criteria as defined by the global variables set earlier in this report).
```{r run Pv3Rs one random indiv, eval=F}
if (RUN_EXAMPLE & unique(indiv_MOI$MOI_summed) <= MAX_MOI_TO_INCLUDE) {
  # Verbose
  cat("Running Pv3Rs for: ", 
      indiv_name, 
      " (summed MOI = ", 
      unique(indiv_MOI$MOI_summed), 
      ").\n", 
      sep = "")
  
  # Finish data preparation
  indiv_haplotype_data <- indiv_haplotype_data %>% 
    group_by(episode_number) %>% 
    group_split() %>% 
    lapply(function(x) {
      res <- x %>% 
        select(sample_id, episode_number, 
               marker_id, haplotype, frequency) %>% 
        # For sensitivity analysis, allow to include/drop allele 
        # based on their within-individual frequency
        filter(frequency >= WITHIN_INDIVIDUAL_ALLELE_FREQ_THR) %>% 
        select(-sample_id, -episode_number, -frequency) %>% 
        distinct() %>% 
        # Prevent dropping of markers that are not characterised 
        # by setting .drop to FALSE
        group_by(marker_id, .drop = FALSE) %>% 
        group_split() %>% 
        lapply(function(y) {
          unique(y$haplotype)
        })
      
      # Returned a list named with each episode, 
      # setting marker allele to NA in case none are observed
      return(lapply(setNames(res, BENCHMARK_MARKERS), 
                    function(y) {
                      if (length(y) == 0) return(NA) else return(y)
                    }))
    })
  
  # Run Aimee's posterior estimation
  indiv_posterior <- Pv3Rs::compute_posterior(y  = indiv_haplotype_data, 
                                              fs = baseline_fs[BENCHMARK_MARKERS])
} else {
  # Verbose
  cat("NOT Running Pv3Rs for: ", 
      indiv_name, 
      " because either summed MOI exceeds threshold (observed = ", 
      unique(indiv_MOI$MOI_summed), 
      ", MAX_MOI_TO_INCLUDE = ", 
      MAX_MOI_TO_INCLUDE, 
      "), or RUN_EXAMPLE was set to FALSE.\n", 
      sep = "")
  
  # Return NULL
  indiv_posterior <- NULL
}

indiv_posterior
```

```{r plot haps for indiv, eval=F}
sols_recurrent_null %>% 
  filter(shuffled_pair_id == indiv_name) %>% 
  ggplot(aes(x = haplotype, y = factor(info_d), fill = factor(haplotype))) +
  geom_tile() +
    facet_grid(~marker_id, scales = "free_x") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```
### Run for all participants
```{r run pv3rs all}
# Start timer
t_start <- Sys.time()

# Initialize an empty list to store the results
indiv_posteriors <- list()

# Loop through each unique individual name
for (indiv_name in unique(analysis_data$subject_id)) {
  # Verbose
  cat("ID : ", indiv_name, "...\n", sep = "")
  
  ## Prepare the data
  # 1- Subset haplotype data to specific individual and apply filters
  indiv_haplotype_data <- analysis_data %>% 
    # Restrict to a single patient
    filter(subject_id == indiv_name) %>% 
    # Ensure episodes are in order
    arrange(episode_number) %>%  
    # Restrict to a subset of markers
    filter(marker_id %in% BENCHMARK_MARKERS) %>% 
    # Restrict to summed MOI below threshold
    group_by(subject_id, episode_number, marker_id) %>% 
    mutate(MOI_per_marker = sum(n())) %>% 
    group_by(subject_id, episode_number) %>% 
    mutate(MOI_per_episode = max(MOI_per_marker, na.rm = TRUE)) %>% 
    ungroup() %>% 
    mutate(marker_id = factor(marker_id, levels = BENCHMARK_MARKERS))
  
  # 2- Calculate per-episode and per-participant MOI for PvR3S eligibility
  indiv_MOI <- indiv_haplotype_data %>% 
    select(subject_id, episode_number, 
           marker_id, starts_with("MOI_")) %>% 
    distinct() %>% 
    group_by(subject_id, episode_number) %>% 
    # Get highest per-marker MOI only for each episode
    # (drop marker_id in case of ties with highest per-marker MOI)
    select(-marker_id) %>% 
    distinct() %>% 
    filter(MOI_per_marker == max(MOI_per_marker, na.rm = TRUE)) %>% 
    ungroup() %>% 
    mutate(MOI_summed = sum(MOI_per_episode, na.rm = TRUE))
  
  ## Run Pv3Rs only if MOI below threshold         
  if (unique(indiv_MOI$MOI_summed) <= MAX_MOI_TO_INCLUDE) {
    # Verbose
    cat("Running Pv3Rs for: ", 
        indiv_name, 
        " (summed MOI = ", 
        unique(indiv_MOI$MOI_summed), 
        ").\n", 
        sep = "")
    
    # Preserve episode numbers for naming the output list
    indiv_episode <- unique(indiv_haplotype_data$episode_number)
    cat("The episode number is: ", indiv_episode) # printing to check episode order number is correct
    
    # Finish data preparation
    indiv_haplotype_data <- indiv_haplotype_data %>% 
      group_by(episode_number) %>% 
      group_split() %>% 
      lapply(function(x) {
        res <- x %>% 
          select(sample_id, episode_number, 
                 marker_id, haplotype, frequency) %>% 
          # For sensitivity analysis, allow to include/drop allele 
          # based on their within-individual frequency
          filter(frequency >= WITHIN_INDIVIDUAL_ALLELE_FREQ_THR) %>% 
          select(-sample_id, -episode_number, -frequency) %>% 
          distinct() %>% 
          # Prevent dropping of markers that are not characterized 
          # by setting .drop to FALSE
          group_by(marker_id, .drop = FALSE) %>% 
          group_split() %>% 
          lapply(function(y) {
            unique(y$haplotype)
          })
        
        # Returned a list named with each episode, 
        # setting marker allele to NA in case none are observed
        return(lapply(setNames(res, BENCHMARK_MARKERS), 
                      function(y) {
                        if (length(y) == 0) return(NA) else return(y)
                      }))
      })
    
    # Run Aimee's posterior estimation
    indiv_posterior <- compute_posterior(y     = indiv_haplotype_data,
                                         fs    = baseline_fs[BENCHMARK_MARKERS], 
                                         prior = matrix(PRIOR_3RS, 
                                                        nrow     = length(indiv_haplotype_data), 
                                                        ncol     = length(PRIOR_3RS), 
                                                        byrow    = TRUE, 
                                                        dimnames = list(c(1:length(indiv_haplotype_data)), 
                                                                        names(PRIOR_3RS))))
    
  } else {
    # Verbose
    cat("NOT Running Pv3Rs for: ", 
        indiv_name, 
        " because summed MOI exceeds threshold (observed = ", 
        unique(indiv_MOI$MOI_summed), 
        ", MAX_MOI_TO_INCLUDE = ", 
        MAX_MOI_TO_INCLUDE, 
        ").\n", 
        sep = "")
    
    # Return NULL
    indiv_episode    <- unique(indiv_haplotype_data$episode_number)
    indiv_posterior <- NULL
  }
  
  # Append the results to the list
  indiv_posteriors[[indiv_name]] <- list("subject_id" = indiv_name, 
                                         "episode_number"       = indiv_episode, 
                                         "Pv3Rs"       = indiv_posterior)
}

# End timer
t_end <- Sys.time()
cat("Pv3Rs for the whole dataset took : ", as.numeric(difftime(time1 = t_end, 
                                                               time2 = t_start, 
                                                               units = "secs"))/60, " mins", "\n", sep = "")

# Present the marginal data in a clearer format
indiv_posteriors_marginal <- do.call(rbind, 
                                     lapply(indiv_posteriors, function(x) {
                                       if (!is.null(x[["Pv3Rs"]])) {
                                         return(data.frame("subject_id"               = x[["subject_id"]], 
                                                           "episode_number"                     = x[["episode_number"]][-1], 
                                                           "Posterior_marginal_prob_C" = x[["Pv3Rs"]]$marg[, "C"], 
                                                           "Posterior_marginal_prob_L" = x[["Pv3Rs"]]$marg[, "L"], 
                                                           "Posterior_marginal_prob_I" = x[["Pv3Rs"]]$marg[, "I"]))
                                       } else {
                                         return(NULL)
                                       }
                                       
                                     }))
row.names(indiv_posteriors_marginal) <- 1:nrow(indiv_posteriors_marginal)

# Present the joint posterior estimates in a clearer format
indiv_posteriors_joint <- do.call(rbind, 
                                  lapply(indiv_posteriors, function(x) {
                                    if (!is.null(x[["Pv3Rs"]])) {
                                      joint_probs <- x[["Pv3Rs"]]$joint
                                      # Extract the state pairs and probabilities
                                      state_pairs <- names(joint_probs)
                                      prob_values <- as.numeric(joint_probs)
                                      
                                      # Create a data frame with subject_id, episode_number, and joint probabilities
                                      return(data.frame("subject_id"      = rep(x[["subject_id"]], length(state_pairs)), 
                                                        "episode_number"  = rep(x[["episode_number"]][-1], each = length(state_pairs)),
                                                        "state_pair"      = state_pairs, 
                                                        "joint_probability" = prob_values))
                                    } else {
                                      return(NULL)
                                    }
                                  }))
row.names(indiv_posteriors_joint) <- 1:nrow(indiv_posteriors_joint)

# Save Pv3Rs output because it's time consuming and 
# we don't want to re-run it every time.
save(list = c("RUN_EXAMPLE", "WITHIN_INDIVIDUAL_ALLELE_FREQ_THR", "BENCHMARK_MARKERS", "MAX_MOI_TO_INCLUDE", "PRIOR_3RS", 
              "analysis_data", "baseline_fs", 
              "indiv_posteriors", "indiv_posteriors_marginal", "indiv_posteriors_joint"), 
     file = paste0("./outputs/Pv3Rs_sols_posteriors_null_distribution_", 
                   strftime(Sys.time(), format = "%Y%m%d_%H%M%S"), 
                   ".RData"))
```

### Explore the posterior
The marginal probabilities give us the probability of the three states for each recurrent episode. However, this does not consider the joint probability of different states when a person experienced more than one recurrent episode. 
```{r marginal probs}
indiv_posteriors_marginal
```

The joint probabilites give us values for each possible combination of states, depending on the number of recurrent episodes experienced by the participant. 
```{r joint probs}
joint_summary <- indiv_posteriors_joint %>% 
                    group_by(subject_id, state_pair, joint_probability) %>% 
                    filter(episode_number == max(episode_number)) %>% 
                    mutate(percentage = round(joint_probability*100, 3),
                           total_recurrences = episode_number-1) %>%
                    select(-episode_number) %>% 
                    arrange(subject_id, total_recurrences, percentage) %>% 
                    relocate(total_recurrences, .before = state_pair)

joint_summary
```

## Plot marginal probabilities 
```{r marg probs}
marginal_summary <- indiv_posteriors_marginal %>% 
  pivot_longer(cols = !subject_id & !episode_number, 
               names_to = "posterior_type", 
               values_to = "posterior_value") %>% 
  mutate(posterior_classification = case_when(posterior_type == "Posterior_marginal_prob_C" ~ "Recrudescence",
                                              posterior_type == "Posterior_marginal_prob_L" ~ "Relapse",
                                              posterior_type == "Posterior_marginal_prob_I" ~ "Reinfection"),
         posterior_classification = factor(posterior_classification, 
                                           levels = c("Relapse", "Recrudescence", "Reinfection"))) %>% 
  select(-posterior_type)

marginal_summary
```

### By shuffled pair ID
```{r plot probabilities, fig.height=8, fig.width=8}
marginal_summary %>% 
  group_by(subject_id) %>%
  arrange(desc(posterior_value), .by_group = TRUE) %>%
  mutate(subject_id = factor(subject_id, levels = unique(subject_id[order(posterior_value, decreasing = TRUE)]))) %>%
  
  ggplot(aes(x = factor(episode_number), y = posterior_value, group = episode_number, fill = posterior_classification)) + 
    geom_bar(stat = "identity", position = "fill") +
    scale_fill_manual(values = c("Relapse" = "turquoise3",
                                 "Recrudescence" = "skyblue4",
                                 "Reinfection" = "magenta3")) +
    scale_x_discrete(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) +
    labs(x     = "Episode number", 
         y     = "Posterior probability", 
         fill = "") + 
    theme_bw() +
    facet_wrap(~subject_id)
```

```{r plot fdr}
marginal_summary %>% 
  group_by(subject_id) %>%
  filter(posterior_classification == "Reinfection") %>% 
  mutate(fdr = 1-posterior_value) %>% 
  ggplot(aes(x = reorder(subject_id, fdr), y = fdr)) +
    geom_col() +
    scale_x_discrete(expand = c(0, 0)) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    labs(x = "Shuffled pair ID", 
         y = "False discovery rate (%)",
         caption = "FDR calculated as 1 - Pr(recrudescence)+Pr(relapse)") + 
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) 
```

## Summarize false discovery rate (FDR)

```{r fdr}
marginal_summary %>% 
  group_by(posterior_classification) %>% 
  summarise(min = min(posterior_value),
            mean = mean(posterior_value),
            max = max(posterior_value))
```

```{r fdr}
marginal_summary %>% 
  group_by(subject_id) %>%
  filter(posterior_value == max(posterior_value)) %>%
  ungroup() %>% 
  tabyl(posterior_classification) %>% 
  adorn_totals()
```

Checking the worst IDs (NewID-46 and NewID-8)
```{r}
sols_recurrent_null %>% 
  filter(shuffled_pair_id == "NewID-46") %>% 
  ggplot(aes(x = haplotype, y = factor(info_d), fill = factor(haplotype))) +
  geom_tile() +
    facet_grid(~marker_id, scales = "free") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

sols_recurrent_null %>% 
  filter(shuffled_pair_id == "NewID-8") %>% 
  ggplot(aes(x = haplotype, y = factor(info_d), fill = factor(haplotype))) +
  geom_tile() +
    facet_grid(~marker_id, scales = "free") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

